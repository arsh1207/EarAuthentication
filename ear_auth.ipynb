{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "# from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "import math\n",
    "from  sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "\n",
    "class Preprocessing:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # this method reads an image and preprocess it by rescaling it to 150 x 100 a applying CLAHE over the image\n",
    "    def read_image(self):\n",
    "        ear_pos = ['down_ear', 'front_ear', 'left_ear', 'up_ear']\n",
    "        person_num = ['000', '001', '002', '003', '004','005','006','007','008','009','010']\n",
    "        images = []\n",
    "        for i in person_num:\n",
    "            for j in ear_pos:\n",
    "                #print(i+'_'+j+\".jpg\")\n",
    "                old_img = cv2.imread(\"EarImages/\"+i+'_'+j+\".jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "                img = old_img[100:620, 40:450]\n",
    "                images.append(cv2.resize(cv2.imread(img, dsize=(100, 150), interpolation=cv2.INTER_NEAREST))\n",
    "\n",
    "        processed_images = []\n",
    "        for i in range(len(images)):\n",
    "            gray = images[i]\n",
    "            mean = cv2.mean(gray)[0]\n",
    "            variance = np.var(gray)\n",
    "            m_t = 100\n",
    "            v_t = 100\n",
    "            for i in range(150):\n",
    "                for j in range(100):\n",
    "                    beta = math.sqrt(v_t * math.pow(gray[i][j] - mean, 2) / variance)\n",
    "                    if gray[i][j] > mean:\n",
    "                        gray[i][j] = m_t + beta\n",
    "                    else:\n",
    "                        gray[i][j] = m_t - beta\n",
    "            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "            cl1 = clahe.apply(gray)\n",
    "            processed_images.append(cl1)\n",
    "        print(processed_images[0].shape)\n",
    "        return processed_images\n",
    "\n",
    "    # in this function we are creating sw of size 50 x 50 with step size 11 so total 50 SWs\n",
    "    def sub_window_creation(self, images, kernels):\n",
    "        gb_all_sw = []\n",
    "        label = []\n",
    "        sub_win_dict = {}\n",
    "        sw_no = 0\n",
    "        count = -1\n",
    "\n",
    "        for i in range(0, 100, 11):\n",
    "            for j in range(0, 50, 11):\n",
    "                count = count +1\n",
    "                for k in range(len(images)):\n",
    "                    image = images[k]\n",
    "                    sw_image = image[i:i+50, j:j+50]\n",
    "\n",
    "                    sw_image = cv2.resize(sw_image, dsize=(12, 12), interpolation=cv2.INTER_NEAREST)\n",
    "                    gabored_image = Preprocessing.process(self, sw_image, kernels)\n",
    "                    \n",
    "                    gb_all_sw.append(gabored_image)   \n",
    "                    label.append(int(k/4))                                                \n",
    "                sw_no +=1\n",
    "        print(len(gb_all_sw))\n",
    "        print(len(gb_all_sw[0]))\n",
    "        # LEM demension reduction\n",
    "        model = SpectralEmbedding(n_components=100, n_neighbors=10)\n",
    "        reduced_sw = model.fit_transform(gb_all_sw)\n",
    "        print('final', reduced_sw[0].shape)\n",
    "        print('final', len(reduced_sw))\n",
    "       # print(label)\n",
    "        sw_no = 0\n",
    "        #adding all Sub Windows to a dict with sub window number as keys\n",
    "        for i in np.arange(0,reduced_sw.shape[0],44): # need to remove hard coded 44 value\n",
    "            red_sw = reduced_sw[i:i+44,:]             #4 images per person, 11 different person, 44 images -> 44 differnt subwindow 1\n",
    "            lbl = np.asarray(label)[i:i+44]\n",
    "            sw_ft_lbl = np.column_stack((red_sw,np.asarray(lbl)))\n",
    "            sub_win_dict[sw_no] = sw_ft_lbl\n",
    "            sw_no += 1\n",
    "        gb_all_sw.clear()\n",
    "        label.clear()\n",
    "        return sub_win_dict\n",
    "\n",
    "    # creating gabor kernel bank\n",
    "    def gabor_filter(self):\n",
    "        kernels = []\n",
    "        for theta in [0,np.pi/4,np.pi/2,np.pi]:\n",
    "            for sigma in [5, 10, 15, 20]:\n",
    "                kernel = np.real(cv2.getGaborKernel((50,50), sigma, theta, 1.0, 0.25, 0, ktype=cv2.CV_32F))\n",
    "                kernel /= 1.5*kernel.sum()\n",
    "                kernels.append(kernel)\n",
    "        return kernels\n",
    "\n",
    "    def process(self, img, filters):\n",
    "        gabored_images = np.array([])\n",
    "        # accum = np.zeros_like(img)\n",
    "        for kern in filters:\n",
    "            fimg = cv2.filter2D(img, cv2.CV_8UC3, kern)\n",
    "            gabored_images = np.append(gabored_images, fimg)\n",
    "        # np.maximum(accum, fimg, accum)\n",
    "        return gabored_images\n",
    "    \n",
    "    def SFFS(self,in_data):\n",
    "        # applies SFFS algorithm for feature selection\n",
    "        selected_sw = []\n",
    "        min_eer = 1\n",
    "        min_eer_2 = 1\n",
    "        prev_min_eer = 1\n",
    "        min_sw_2 = 0\n",
    "    \n",
    "        while(len(selected_sw)<5): # 5 -> Number of Subwindows to select\n",
    "            min_sw = 0\n",
    "            for (sw_name, sw_data) in in_data.items(): #step 1 of SFFS algorithm\n",
    "                #print(sw_name)\n",
    "                Xk = sw_data[:,:-1]\n",
    "                label = sw_data[:,-1]\n",
    "                label = label_binarize(label, classes = np.arange(0,11,1)) # multi class problem hence binarizing labels for easy ROC variable calculation\n",
    "                if(sw_name not in selected_sw):\n",
    "                    for sw_no in selected_sw:\n",
    "                        sw_data = in_data[sw_no]\n",
    "                        Xk = np.column_stack((Xk,sw_data[:,:-1]))\n",
    "                    #print(Xk.shape)                \n",
    "                    X_train,X_valid,y_train,y_valid = train_test_split(Xk,label,test_size=0.25)\n",
    "                    classifier = OneVsRestClassifier(KNeighborsClassifier(n_neighbors = 1))                \n",
    "                    classifier.fit(X_train,y_train)\n",
    "                    y_pred =classifier.predict(X_valid)\n",
    "                    (all_fpr,all_tpr,treshold) = roc_curve(y_valid.ravel(),y_pred.ravel())                \n",
    "                    all_fnr = 1-all_tpr\n",
    "                    difference = np.abs(all_fpr-all_fnr)\n",
    "               \n",
    "                    min_dif_index = np.argmin(difference)\n",
    "    \n",
    "                    EER = np.mean((all_fpr[min_dif_index],all_fnr[min_dif_index]))\n",
    "\n",
    "                    if(EER < min_eer):\n",
    "                        min_eer = EER\n",
    "                        #selected_sw.append(sw_name)\n",
    "                        min_sw = sw_name\n",
    "            print(min_eer)\n",
    "            print(\"Step 1 selected\")\n",
    "            print(min_sw)\n",
    "            selected_sw.append(min_sw)\n",
    "            if(len(selected_sw) >1): #step 2 of SFFS algorithm\n",
    "                for sw in selected_sw:\n",
    "                    Xk = np.zeros_like(in_data[sw][:,:-1])\n",
    "                    for sw_int in selected_sw:\n",
    "                        if(sw_int != sw):\n",
    "                            Xk = np.column_stack((Xk,(in_data[sw_int])[:,:-1]))\n",
    "                    yk = in_data[sw][:,-1]\n",
    "                    Xkf = Xk[:,100:]\n",
    "                    #print(Xkf.shape)\n",
    "                    ykf = label_binarize(yk, classes = np.arange(0,11,1))\n",
    "                    X_train,X_valid,y_train,y_valid = train_test_split(Xkf,ykf,test_size=0.25)\n",
    "                    classifier = OneVsRestClassifier(KNeighborsClassifier(n_neighbors = 1))\n",
    "                    classifier.fit(X_train,y_train)\n",
    "                    classifier.predict(X_valid)\n",
    "                    (all_fpr,all_tpr,treshold) = roc_curve(y_valid.ravel(),y_pred.ravel())\n",
    "                    all_fnr = 1-all_tpr\n",
    "                    difference = np.abs(all_fpr-all_fnr)\n",
    "               # print(difference)\n",
    "                    min_dif_index = np.argmin(difference)\n",
    "                    EER = np.mean((all_fpr[min_dif_index],all_fnr[min_dif_index]))\n",
    "                    if(EER < min_eer_2):\n",
    "                        min_eer_2 = EER\n",
    "                        min_sw_2 = sw\n",
    "                if(min_eer_2 <= prev_min_eer):\n",
    "                    selected_sw.remove(min_sw_2)\n",
    "                    prev_min_eer = min_eer_2\n",
    "                    print(\"Step 2 removed\")\n",
    "                    print(min_sw_2)\n",
    "                else:\n",
    "                    prev_min_eer = min_eer\n",
    "                min_eer = 1  \n",
    "                min_eer_2 = 1\n",
    "      #  print(selected_sw)\n",
    "      #  print(prev_min_eer)\n",
    "        return selected_sw\n",
    "    \n",
    "    def create_matchers(self,inp_data,in_labels):\n",
    "        classifier =[]\n",
    "        for i in np.arange(0,inp_data.shape[1],100):\n",
    "            print(\"Creating the Matcher : %d\" % i)\n",
    "            sw_data = inp_data[:,i:i+100]\n",
    "            clf = KNeighborsClassifier(n_neighbors = 1)\n",
    "            clf.fit(sw_data,in_labels)\n",
    "            classifier.append(clf)\n",
    "        return classifier\n",
    "    \n",
    "    def fusion_test(self,classifiers,test_data,test_results):\n",
    "        c_no =0\n",
    "        #ret_prediction =np.array\n",
    "        for i in np.arange(0,test_data.shape[1],100):\n",
    "            classifier = classifiers[c_no]\n",
    "            pred_prob = classifier.predict(test_data[:,i:i+100])\n",
    "            print(pred_prob)\n",
    "            print(test_results)\n",
    "            c_no += 1\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 100)\n",
      "2200\n",
      "2304\n",
      "final (100,)\n",
      "final 2200\n",
      "(44, 101)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sadiq\\Miniconda3\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 2 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\sadiq\\Miniconda3\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 3 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35\n",
      "Step 1 selected\n",
      "33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sadiq\\Miniconda3\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 2 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n",
      "Step 1 selected\n",
      "42\n",
      "Step 2 removed\n",
      "33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sadiq\\Miniconda3\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 3 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\sadiq\\Miniconda3\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 4 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\sadiq\\Miniconda3\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 3 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\sadiq\\Miniconda3\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 2 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n",
      "Step 1 selected\n",
      "6\n",
      "0.3\n",
      "Step 1 selected\n",
      "32\n",
      "0.4\n",
      "Step 1 selected\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sadiq\\Miniconda3\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 0 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\sadiq\\Miniconda3\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 8 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\sadiq\\Miniconda3\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 9 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\sadiq\\Miniconda3\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 7 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n",
      "Step 1 selected\n",
      "4\n",
      "[42, 6, 32, 0, 4]\n"
     ]
    }
   ],
   "source": [
    "def select_classification_data(sw_dict, chosen_sw):\n",
    "    combined_data = sw_dict[chosen_sw[0]][:,:-1]\n",
    "    label = sw_dict[chosen_sw[0]][:,-1]\n",
    "    for i in range(1,len(chosen_sw)):\n",
    "        sw_no = chosen_sw[i]\n",
    "        combined_data = np.column_stack((combined_data,sw_dict[sw_no][:,:-1]))\n",
    "    X_train,X_test,y_train,y_test = train_test_split(combined_data, label,test_size=0.2)\n",
    "    return {\"xtrain\":X_train, \"xtest\":X_test, \"ytrain\":y_train, \"ytest\":y_test}\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "obj = Preprocessing()\n",
    "#preprocess the given image\n",
    "processed_image = obj.read_image()\n",
    "# print(len(processed_image))\n",
    "# print(processed_image[0].shape)\n",
    "#create gabor filter bank\n",
    "kernel_bank = obj.gabor_filter()\n",
    "#feature extraction and transformation\n",
    "sub_windows_dict = obj.sub_window_creation(processed_image, kernel_bank)\n",
    "print(sub_windows_dict[0].shape)\n",
    "#feature selection\n",
    "chosen_sw = obj.SFFS(sub_windows_dict)\n",
    "print(chosen_sw)\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the Matcher : 0\n",
      "Creating the Matcher : 100\n",
      "Creating the Matcher : 200\n",
      "Creating the Matcher : 300\n",
      "Creating the Matcher : 400\n",
      "[8. 2. 8. 1. 8. 9. 5. 8. 8.]\n",
      "[6. 3. 6. 4. 5. 4. 3. 8. 6.]\n",
      "[ 0. 10.  1.  2. 10.  0.  0.  4.  0.]\n",
      "[6. 3. 6. 4. 5. 4. 3. 8. 6.]\n",
      "[7. 7. 8. 5. 1. 9. 1. 1. 1.]\n",
      "[6. 3. 6. 4. 5. 4. 3. 8. 6.]\n",
      "[10. 10.  0.  1.  0.  0.  6.  2. 10.]\n",
      "[6. 3. 6. 4. 5. 4. 3. 8. 6.]\n",
      "[7. 1. 3. 6. 2. 9. 7. 8. 7.]\n",
      "[6. 3. 6. 4. 5. 4. 3. 8. 6.]\n"
     ]
    }
   ],
   "source": [
    "classification_data = select_classification_data(sub_windows_dict,chosen_sw)\n",
    "#Generating matchers\n",
    "matchers = obj.create_matchers(classification_data[\"xtrain\"],classification_data[\"ytrain\"])\n",
    "#fusion of matchers\n",
    "obj.fusion_test(matchers,classification_data[\"xtest\"],classification_data[\"ytest\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
